% !TEX root = ../doc.tex

\section{Introduction}

Software errors are a significant trigger for financial damage and damage to the reputation of companies. Such errors range from minor bugs to serious security vulnerabilities. For this reason, there is a great deal of interest in warning a developer when they release updated software code that may contain one or more bugs. To this end, researchers and software developers have developed various techniques for error detection and error prediction over the past decade, which are largely based on methods and techniques of Machine Learning \cite{Challagulla2008}. These techniques usually use historical data of defective and defect-free changes to software systems in combination with a carefully compiled set of attributes (usually called features\footnote{To avoid ambiguous and misleading use of the feature term, the term "attribute" will be used throughout this paper to describe the characteristics of data.}) to train a given classifier \cite{Alsaeedi2019,Hammouri2018}. This can then be used to make an accurate prediction of whether a new change to a piece of software is defective or clean. The choice of algorithms for classification is large. Studies show that, out of the pool of available algorithms, both tree-based (e.g. J48, CART or Random Forest) and Bayesian algorithms (e.g. Na\"{\i}ve Bayes (NB), Bernoulli-NB or multinomial NB) are the most widely used \cite{Son2019}. Alternative algorithms include regression, k-nearest-neighbors or artificial neural networks \cite{Challagulla2008}. It should be noted, however, that there is no consensus on the best available algorithm, since each algorithm has different strengths and weaknesses for specific use cases. The goal of this work is to develop such a prediction technique for software errors based on software features. These features describe increments of the functionality of a software system. The software systems developed in this way are called software product lines and consist of a set of similar software products. They are characterized by having a common set of features and a common code base \cite{Thuem2014}. By having different features along the software products, a wide variability within a product line can be achieved. In the development of the prediction technique, the implementation of features is performed by means of preprocessor instructions, such as \texttt{\#IFDEF} and \texttt{\#IFNDEF} (also called preprocessor directives). This approach \cite{Queiroz2016}, which has so far only been considered once in a case study, is promising for several reasons:

\begin{enumerate}
\setlength{\itemsep}{-2pt}
\item Since a given feature  might be historically more or less error-prone, a change that updates  the feature may be more or less eror-prone as well.
\item Features more or  less likely to be error-prone might have certain characteristics that can be harnessed for defect predictoin.
\item Code that contains a lot of feature-specific code (specifically, the so-called feature interactions) might be more error-prone than others. 
\end{enumerate}

The above-mentioned goal of the work consists of several sub-goals. Among them is the creation of a dataset including the feature aspect. This dataset in turn serves to train a representative selection of classifiers with a subsequent comparative evaluation of these. In addition, the feature-based data set is compared with a classical file-based data set, the development of which was taken from a scientific paper \cite{Moser2008}.
